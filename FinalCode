# Install dependencies
!pip install ultralytics scikit-learn opencv-python-headless matplotlib

import cv2
import numpy as np
from ultralytics import YOLO
from sklearn.cluster import KMeans
from matplotlib import colors
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow  # for displaying images in Colab

# Load YOLOv8 model
model = YOLO('yolov8n.pt')

# Function to calculate speed
def calculate_speed(distance, time_interval):
    return distance / time_interval

# Function to get the dominant color
def get_dominant_color(image, k=3):
    if image.size == 0:
        return (0, 0, 0)
    image = cv2.resize(image, (250, 250))
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    pixels = image.reshape((-1, 3))
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(pixels)
    counts = np.bincount(kmeans.labels_)
    dominant = kmeans.cluster_centers_[np.argmax(counts)]
    return tuple(dominant.astype(int))

# Function to convert RGB to closest color name
def rgb_to_color_name(rgb_color):
    css_colors = colors.CSS4_COLORS
    min_dist = float('inf')
    closest_color = None
    for name, hex in css_colors.items():
        r, g, b = colors.to_rgb(hex)
        r, g, b = int(r * 255), int(g * 255), int(b * 255)
        dist = np.sqrt((r - rgb_color[0])*2 + (g - rgb_color[1])2 + (b - rgb_color[2])*2)
        if dist < min_dist:
            min_dist = dist
            closest_color = name
    return closest_color

# Function to detect additional features (e.g., specific vehicle attributes)
def detect_additional_features(vehicle_image):
    # Placeholder for detecting other features
    features = {
        'feature_1': None,  # Replace with actual feature detection logic
        'feature_2': None,  # Replace with actual feature detection logic
    }
    return features

# Define the video processing function
def process_video(input_video_path, output_video_path):
    # Open video
    cap = cv2.VideoCapture(input_video_path)
    if not cap.isOpened():
        print(f"Error: Cannot open video {input_video_path}")
        return

    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    # Set the output video format
    video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))

    # Initialize counters
    vehicle_speeds = {}
    car_count, truck_count, bus_count = 0, 0, 0
    distance_per_pixel = 0.05  # Assuming distance per pixel
    time_interval = 1 / fps

    # Dictionary to store detected features (not shown on the video)
    detected_features = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        results = model(frame)

        # Vehicle speed detection and counting
        if results[0].boxes is not None:
            for box in results[0].boxes:
                cls_id = int(box.cls)
                label = model.model.names[cls_id]

                # Draw bounding box
                bbox = box.xyxy[0].numpy().astype(int)
                cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)

                # Calculate speed
                distance = distance_per_pixel * 5
                speed = calculate_speed(distance, time_interval)

                # Count vehicles
                if label == 'car':
                    car_count += 1
                elif label == 'truck':
                    truck_count += 1
                elif label == 'bus':
                    bus_count += 1

                # Store speed
                if label not in vehicle_speeds:
                    vehicle_speeds[label] = []
                vehicle_speeds[label].append(speed)

                # Get dominant color
                car_img = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]
                dominant_color = get_dominant_color(car_img)
                color_name = rgb_to_color_name(dominant_color)

                # Detect additional features but do not display them
                additional_features = detect_additional_features(car_img)
                detected_features.append({
                    'label': label,
                    'speed': speed,
                    'color': color_name,
                    'bbox': bbox,
                    'additional_features': additional_features
                })

                # Draw vehicle type, speed, and color (output features)
                cv2.putText(frame, f"{label}: {speed:.2f} m/s, Color: {color_name}", (bbox[0], bbox[1] - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

        # Display counts
        cv2.putText(frame, f'Cars: {car_count}', (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        cv2.putText(frame, f'Trucks: {truck_count}', (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        cv2.putText(frame, f'Buses: {bus_count}', (20, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Write the modified frame to the output video file
        video_writer.write(frame)

    # Release resources
    cap.release()
    video_writer.release()

    # Optionally, save detected features to a file or print them
    for feature in detected_features:
        print(feature)

# Upload video files to Google Colab
from google.colab import files
uploaded = files.upload()

# List uploaded files
input_video_1 = list(uploaded.keys())[0]
output_video_1 = "output_video_1.mp4"
output_video_2 = "output_video_2.mp4"

# Process videos
process_video(input_video_1, output_video_1)
# Assuming you upload two videos, otherwise you can just process the first one
# process_video(input_video_2, 'output_video_2.mp4')

# To download the output video
from google.colab import files
files.download(output_video_1)
